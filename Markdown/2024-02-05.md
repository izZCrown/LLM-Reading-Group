Uncertainty of LLMs


李浩东


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_1_1.png" width="1160" height="559"/>


Uncertainty of DNNs


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_2_1.png" width="605" height="275"/>


Calibration


Estimation


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_2_2.png" width="421" height="285"/>


 Expected Calibration Error（ECE）


Using model logits for LLMs presents several limitations：

	1） logits imply overconfidence in many cases

	2） logits only capture the model’s uncertainty regarding next the token rather than 
	       providing an assessment of the reliability of a specific claim, which is the 
                    behavior desired in human-like responses.

	3） the rise of closed-source LLMs, such as GPT-3.5 and GPT-4 with commercialized 
	      APIs only allowing textual inputs and outputs, lacks access to model logits or 
	      embeddings. 


Introduction


Uncertainty estimation method for LLMs


Calibration: 
	Temperature scaling, label smoothing and knowledge distillation.

Conformal Prediction

Verbalised Uncertainty

Other: 
	Ensemble models


Open source LLM


Ensemble Models


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_6_1.png" width="1119" height="66"/>


[Uncertainty Estimation for Language Reward Models]


Shortcoming: compute resources and time


初始参数不同
微调轮次
微调的数据是从同一数据集中分割的
……


Teaching models to express their uncertainty in words


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_7_1.png" width="744" height="182"/>





<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_7_2.png" width="881" height="317"/>


 3 Kinds of Probability


The Internal State of an LLM Knows When its Lying


 The True-False Dataset


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_8_1.png" width="946" height="396"/>


Conformal Prediction


CP是一种使用模型预测准确率的方法，大致过程如下：
使用训练集训练出一个模型，此处简称原模型
使用训练集的数据以及原模型对训练集作出的预测相结合作为训练集，训练出一个新的模型（此处称为CP模型）
CP模型的输入特征可以与原模型的输入不同（两个模型的特征相关但不一定要完全一致）
CP模型的输出是1个置信度和1个不一致程度。CP模型直接使用新模型去拟合原模型输出是否与真实y值不一致的程度（如直接拟合差值的绝对值等，原论文中未出现明确的方法）
然后会通过公式将该值映射到0~1之间的值：


可以看到，该映射将α按样本的数量等距映射到0~1之间，并且α的值越大，映射出的p值越小；因为α是不一致性，那么p值就用来衡量一致性的大小。

经过以上步骤，我们得到了一个可以判断原模型是否准确的CP模型。


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_9_1.png" width="464" height="75"/>


Black-box LLM


Can LLMs Express Their Uncertainty?  An Empirical Evaluation of Confidence Elicitation in LLMs


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_11_1.png" width="1265" height="385"/>


Can LLMs Express Their Uncertainty?  An Empirical Evaluation of Confidence Elicitation in LLMs


Confidence Elicitation Methods —— Verbalized Confidence


vanilla verbalized confidence


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_12_1.png" width="1091" height="358"/>


Can LLMs Express Their Uncertainty?  An Empirical Evaluation of Confidence Elicitation in LLMs


Confidence Elicitation Methods —— Verbalized Confidence


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_13_1.png" width="1157" height="473"/>


Can LLMs Express Their Uncertainty?  An Empirical Evaluation of Confidence Elicitation in LLMs


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_14_1.png" width="716" height="465"/>


Can LLMs Express Their Uncertainty?  An Empirical Evaluation of Confidence Elicitation in LLMs


Consistency-based Confidence —— self-consistency 


 


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_15_1.png" width="1131" height="272"/>


Consistency-based Confidence —— Induced Consistency 


GENERATING WITH CONFIDENCE: UNCERTAINTY QUANTIFICATION FOR 
BLACK-BOX LARGE LANGUAGE MODELS


For Natural Language Generation (NLG)


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_16_1.png" width="1010" height="107"/>


MEASURING RESPONSE SIMILARITIES
Jaccard Similarity


		

Natural Language Inference (NLI)


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_16_2.png" width="543" height="45"/>


DeBERTalarge model


GENERATING WITH CONFIDENCE: UNCERTAINTY QUANTIFICATION FOR 
BLACK-BOX LARGE LANGUAGE MODELS


For Natural Language Generation (NLG)


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_17_1.png" width="1010" height="107"/>


 ESTIMATING UNCERTAINTY  FROM SIMILARITIES
Semantic Sets 


QUANTIFYING UNCERTAINTY IN ANSWERS FROM ANY LANGUAGE MODEL AND ENHANCING THEIR TRUSTWORTHINESS


Our confidence assessment derives from two factors: 
	Observed Consistency and Self-reflection Certainty,
	which respectively are extrinsic and intrinsic evaluations of LLM confidence.


 OBSERVED CONSISTENCY
	The first critical measure of model uncertainty is contradiction score amongst possible answers LLMs
	gives to a particular input questions.
increasing the temperature values
modify the prompt 
	
SELF-REFLECTION CERTAINTY



<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_18_1.png" width="808" height="135"/>


Truthfulness：calibration uncertainty of LLM 

Honesty: error detection
	      Out of Distribution Detection

 Robust training: Active Learning

Evaluate LLM


Applications

