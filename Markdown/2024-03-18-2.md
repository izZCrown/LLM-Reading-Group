Scaling Autoregressive Models for Content-Rich Text-to-Image Generation

Ho, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku, A., Yang, Y., Ayan, B. K., Hutchinson, B., Han, W., Parekh, Z., Li, X., Zhang, H., Baldridge, J., & Wu, Y.

arXiv preprint arXiv:2206.10789


![img](./Markdown/Images/2024-03-18-2/img_1_1.png)


Overview


![img](./Markdown/Images/2024-03-18-2/img_2_1.png)


Super-Resolution


![img](./Markdown/Images/2024-03-18-2/img_3_1.png)


参数量


![img](./Markdown/Images/2024-03-18-2/img_4_1.png)


Classifier-Free Guidance and Reranking


![img](./Markdown/Images/2024-03-18-2/img_4_2.png)


Training Datasets


The data includes the publicly available LAION-400M dataset [43]; FIT400M, a filtered subset of the full 1.8 billion examples used  to train the ALIGN model [9]; JFT-4B dataset [44], which has images with text annotation labels. 

For textual descriptions of JFT, we randomly switch between the original labels as text (concatenated if an image has multiple labels) or machine-generated captions from a SimVLM model [45].


PartiPrompts


![img](./Markdown/Images/2024-03-18-2/img_6_1.png)


![img](./Markdown/Images/2024-03-18-2/img_6_2.png)


PartiPrompts


![img](./Markdown/Images/2024-03-18-2/img_7_1.png)


评估测试


![img](./Markdown/Images/2024-03-18-2/img_8_1.png)


评估测试


![img](./Markdown/Images/2024-03-18-2/img_9_1.png)


评估测试


![img](./Markdown/Images/2024-03-18-2/img_10_1.png)


缺点


![img](./Markdown/Images/2024-03-18-2/img_11_1.png)


缺点


![img](./Markdown/Images/2024-03-18-2/img_12_1.png)


Improving Image Generation with Better Captions

James Betker, Gabriel Goh, Li Jing, Tim Brooks, JianfengWang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao, Aditya Ramesh

https://cdn.openai.com/papers/dall-e-3.pdf


Building an image captioner


![img](./Markdown/Images/2024-03-18-2/img_14_1.png)


Fine-tuning the captioner


![img](./Markdown/Images/2024-03-18-2/img_15_1.png)


Caption type results


![img](./Markdown/Images/2024-03-18-2/img_16_1.png)


![img](./Markdown/Images/2024-03-18-2/img_16_2.png)


Caption blending ratios


![img](./Markdown/Images/2024-03-18-2/img_17_1.png)


DALL-E 3与其他模型的对比


![img](./Markdown/Images/2024-03-18-2/img_18_1.png)


![img](./Markdown/Images/2024-03-18-2/img_18_2.png)


Zero-Shot Text-to-Image Generation

Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, & Ilya Sutskever

arXiv preprint arXiv:2102.12092


Model detail


![img](./Markdown/Images/2024-03-18-2/img_20_1.png)


![img](./Markdown/Images/2024-03-18-2/img_20_2.png)


![img](./Markdown/Images/2024-03-18-2/img_20_3.png)


Method


![img](./Markdown/Images/2024-03-18-2/img_21_1.png)


![img](./Markdown/Images/2024-03-18-2/img_21_2.png)


Technical details


![img](./Markdown/Images/2024-03-18-2/img_22_1.png)


![img](./Markdown/Images/2024-03-18-2/img_22_2.png)


Experiments


![img](./Markdown/Images/2024-03-18-2/img_23_1.png)


![img](./Markdown/Images/2024-03-18-2/img_23_2.png)


Experiments


![img](./Markdown/Images/2024-03-18-2/img_24_1.png)

