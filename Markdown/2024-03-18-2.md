Scaling Autoregressive Models for Content-Rich Text-to-Image Generation

Ho, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku, A., Yang, Y., Ayan, B. K., Hutchinson, B., Han, W., Parekh, Z., Li, X., Zhang, H., Baldridge, J., & Wu, Y.

arXiv preprint arXiv:2206.10789


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_1_1.png" width="1006" height="513"/>


Overview


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_2_1.png" width="1007" height="348"/>


Super-Resolution


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_3_1.png" width="1041" height="288"/>


参数量


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_4_1.png" width="850" height="85"/>


Classifier-Free Guidance and Reranking


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_4_2.png" width="850" height="275"/>


Training Datasets


The data includes the publicly available LAION-400M dataset [43]; FIT400M, a filtered subset of the full 1.8 billion examples used  to train the ALIGN model [9]; JFT-4B dataset [44], which has images with text annotation labels. 

For textual descriptions of JFT, we randomly switch between the original labels as text (concatenated if an image has multiple labels) or machine-generated captions from a SimVLM model [45].


PartiPrompts


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_6_1.png" width="569" height="532"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_6_2.png" width="591" height="496"/>


PartiPrompts


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_7_1.png" width="1167" height="459"/>


评估测试


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_8_1.png" width="930" height="429"/>


评估测试


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_9_1.png" width="1034" height="269"/>


评估测试


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_10_1.png" width="715" height="464"/>


缺点


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_11_1.png" width="1197" height="513"/>


缺点


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_12_1.png" width="1172" height="506"/>


Improving Image Generation with Better Captions

James Betker, Gabriel Goh, Li Jing, Tim Brooks, JianfengWang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao, Aditya Ramesh

https://cdn.openai.com/papers/dall-e-3.pdf


Building an image captioner


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_14_1.png" width="975" height="471"/>


Fine-tuning the captioner


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_15_1.png" width="739" height="562"/>


Caption type results


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_16_1.png" width="184" height="64"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_16_2.png" width="969" height="365"/>


Caption blending ratios


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_17_1.png" width="894" height="468"/>


DALL-E 3与其他模型的对比


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_18_1.png" width="1035" height="263"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_18_2.png" width="1035" height="248"/>


Zero-Shot Text-to-Image Generation

Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, & Ilya Sutskever

arXiv preprint arXiv:2102.12092


Model detail


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_20_1.png" width="990" height="527"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_20_2.png" width="636" height="175"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_20_3.png" width="482" height="176"/>


Method


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_21_1.png" width="474" height="497"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_21_2.png" width="756" height="490"/>


Technical details


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_22_1.png" width="1018" height="304"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_22_2.png" width="998" height="270"/>


Experiments


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_23_1.png" width="520" height="379"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_23_2.png" width="717" height="535"/>


Experiments


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_24_1.png" width="1154" height="552"/>

