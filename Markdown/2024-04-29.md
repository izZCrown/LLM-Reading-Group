![img](./Markdown/Images/2024-04-29/img_0_1.png)




















Privacy Leaks in 
Large Language Models for Code


![img](./Markdown/Images/2024-04-29/img_0_2.png)


1


BACKGROUND


Training Data Exposure:
If private code is used without permission, the model might inadvertently "memorize" specific portions of these codes, leading to privacy leaks.

Sensitive Information in Generated Code:
When LLMs for Code generate code, they might include patterns and information based on their training data. If this data contains sensitive elements like API keys, passwords, or other confidential data, the model might unintentionally include this information in its outputs.

Inference Attacks:
Attackers can potentially manipulate the model to reveal sensitive information from its training data through carefully crafted queries. For example, submitting specific code snippets to the model could allow attackers to infer details about other related code or data.


What is Privacy Leakage in LLM for Code?


2





CodexLeaks:
Privacy Leaks from Code Generation Language Models in GitHub Copilot


Part 01





Liang Niu, Shujaat Mirza, Zayd Maradni, Christina P√∂pper

In 32nd USENIX Security Symposium (USENIX Security 23). USENIX Association, Anaheim, CA, 2133‚Äì2150.


3


INTRODUCTION


Existing works on regurgitation of training data and resulting privacy leaks have mostly focused on evaluating general-purpose language models pre-trained for English language text generation.


4


Shortcomings of existing research for Codex family models


functionality


security


Verbatim memorization defense efficacy


privacy


OVERVIEW


5


CodexLeaks pipeline
A semi-automated pipeline to extract sensitive personal information from the Codex model.

Templates
We develop templates to generate prompts for diverse categories of personal information to query the model with, and perform prompt-specific temperature tuning.

BlindMI
We then customize a blind membership inference (BlindMI) technique, based on differential comparisons that automatically filters non-leakage from output responses.


OVERVIEW


CodexLeaks pipeline
We construct prompts based on three construction methods, then query the Codex language model with those prompts, and filter the generated code snippets using membership inference before further evaluating the extracted leak candidates.


6


![img](./Markdown/Images/2024-04-29/img_5_1.png)


APPROACH


Privacy Leaks

r = f(p)

We label it as a privacy leak if it contains personal Information that is deemed memorized ‚Äîverbatim or partial.

Treat Model
An attacker only has input-output access to the model.
The attacker can also control the temperature hyperparameter. 
The attackers may have partial access to code sequences from the training data.


7


APPROACH


Prompt Construction

Handcrafted construction


8


![img](./Markdown/Images/2024-04-29/img_7_1.png)


![img](./Markdown/Images/2024-04-29/img_7_2.png)


![img](./Markdown/Images/2024-04-29/img_8_1.png)


APPROACH


Prompt Construction

Template-based construction


9


![img](./Markdown/Images/2024-04-29/img_8_2.png)


"name": "David","Facebook": "


"{{language.name}}": "{{context.people_name}}", "{{language.sns}}": "


APPROACH


Prompt Construction

GitHub sampling based construction

These prompts usually come with realistic details and context.



e.g. ‚ÄúdateOfBirth‚Äù:‚Äú2020-01-15‚ÄùÔºå\n ‚ÄúpassportDetails‚Äù:{\n ‚ÄúpassportNumber‚Äù:‚Äù


10


![img](./Markdown/Images/2024-04-29/img_9_1.png)


APPROACH


Parameter Tuning


account.password =‚Äú


user.name="XXX" \n user.email="XXX@gmail.com" \n user.password=‚Äú


account.password =‚Äú10



11


![img](./Markdown/Images/2024-04-29/img_10_1.png)


![img](./Markdown/Images/2024-04-29/img_10_2.png)


APPROACH


Automatic Filtering using BlindMI

Log probabilities

Subsequence length
The subsequence with low perplexity

Features
log-prop-sorted
log-prop-unsorted
perplexity
multi-perplexity (0.1 or 0.2)
3-gram or 5-gram
0.5 or 0.75 or 0.9




12


![img](./Markdown/Images/2024-04-29/img_11_1.png)


![img](./Markdown/Images/2024-04-29/img_11_2.png)


EVALUATION


Targeted leaks

A response is classified as a leak if there is a clear connection between the subject of the input prompt and the personal information disclosed in the output response.

Indirect leaks

We also label an output response as a leak if the information contained is valid and belongs to an individual other than the subject of the prompt.

Uncategorized leaks

In cases where we cannot verify information, the absence of search results does not guarantee non-memorization.



13


EVALUATION


14


BlindMI on StarCoder„ÄÅPolyCoder and CodeParrot


![img](./Markdown/Images/2024-04-29/img_13_1.png)


![img](./Markdown/Images/2024-04-29/img_13_2.png)


EVALUATION


15


![img](./Markdown/Images/2024-04-29/img_14_1.png)





EVALUATION


16


![img](./Markdown/Images/2024-04-29/img_15_1.png)


![img](./Markdown/Images/2024-04-29/img_15_2.png)





Gotcha! This Model Uses My Code!
Evaluating Membership Leakage Risks in Code Models


Part 02





Zhou Yang, Zhipeng Zhao, Chenyu Wang, Jieke Shi, Dongsun Kim, DongGyun Han, David Lo

 


17


OVERVIEW


18


‚Ä¢ MIA threats in code models: 
We are the first to investigate the risks of membership information leakage when using codemodels. We propose Gotcha, an effective membership inference attack method for code models to investigate such risks. 

‚Ä¢ Risk assessment of code models: 
The attacker‚Äôs knowledge of the victim model affects the risk of membership information leakage. Using a different decoding strategy (i.e., changing from beam-search to top-ùëò sampling) can mitigate the risk.

‚Ä¢ Empirical study: 
We find that the risk of leaking pre-training data‚Äôs membership information is relatively lower. 


OVERVIEW


19


ModelÔºö

codeGPT(beam search)

Threat Model: 

Assumption 1:
The users of code models have black-box access to the models multiple times to collect the pairs of
input and output. 

Assumption 2: 
The attacker cannot access the model parameters or the gradient information.

Assumption 3:
The users can access part of the training data of the models.


APPROACH


20


Task FormulationÔºö

The model M can be queried and complete code in a black-box manner.



The attacker aims to build a binary classifier G to infer whether an example (ùë•, ùë¶) is a member of the training set Dùëñùëõ .


![img](./Markdown/Images/2024-04-29/img_19_1.png)


![img](./Markdown/Images/2024-04-29/img_19_2.png)


![img](./Markdown/Images/2024-04-29/img_19_3.png)


x


y





![img](./Markdown/Images/2024-04-29/img_19_4.png)


M


APPROACH


21


Training Surrogate ModelsÔºö

MÔºövictim model
SÔºösurrogate model


![img](./Markdown/Images/2024-04-29/img_20_1.png)





![img](./Markdown/Images/2024-04-29/img_20_2.png)


Strain





![img](./Markdown/Images/2024-04-29/img_20_3.png)


S


(x, y)


(x, y,       )


![img](./Markdown/Images/2024-04-29/img_20_4.png)


APPROACH


22


Training MIA ClassifiersÔºö


x


y





![img](./Markdown/Images/2024-04-29/img_21_1.png)


CodeBERT
enbedding


ùêø√ó768 x


ùêø√ó768 y


ùêø√ó768 y


![img](./Markdown/Images/2024-04-29/img_21_2.png)


Average pooling


1√ó768 x


1√ó768 y


1√ó768 y


![img](./Markdown/Images/2024-04-29/img_21_3.png)


x


y





![img](./Markdown/Images/2024-04-29/img_21_4.png)


MIA Classifier


![img](./Markdown/Images/2024-04-29/img_21_5.png)


![img](./Markdown/Images/2024-04-29/img_21_6.png)





EVALUATION


True Positive Rate (TPR)

The true positive rate represents the ability of the attacker to correctly identify all the instances that are part of the training dataset. 

False Positive Rate (FPR)
The false positive rate quantifies the rate at which the attacker mistakenly identifies instances that are not part of the training dataset. 

Area Under the ROC Curve (AUC)
The Area Under the Curve (AUC) is a single numerical value derived from the ROC curve that summarizes the overall performance of the attacker. 


23


EVALUATION


Victim Model
CodeGPT(microsoft/CodeGPT-small-java)‚Äî‚Äîpre-trainedon CodeSearchNet, fine-tuned on JavaCorpus

Datasets
JavaCorpus(1%), 12,934/7,189/8,268 files for the training/validation/test set.

Baselines
Approaches by Hisamoto et al. (Hisamoto et al., 2020) and Carlini et al. (Carlini et al., 2021). 

Metrics-based Ranking
Perplexity„ÄÅComparing perplexity of another language modelÔºà         Ôºâ„ÄÅComparing to zlib compressionÔºà          Ôºâ.


24


![img](./Markdown/Images/2024-04-29/img_23_1.png)


![img](./Markdown/Images/2024-04-29/img_23_2.png)


EVALUATION


RQ1. To what extent are code models vulnerable to membership inference attacks?
We assume that the attacker knows 20% of the training data of the victim mode. 


25


![img](./Markdown/Images/2024-04-29/img_24_1.png)


![img](./Markdown/Images/2024-04-29/img_24_2.png)


EVALUATION


RQ2. What are the factors affecting the membership leakage risk?
The number of training epochs of the victim model has little impact on the risk of membership leakage. However, the risk is higher if an attacker knows the victim model better, e.g., the model‚Äôs architecture and training data.


26





![img](./Markdown/Images/2024-04-29/img_25_1.png)


![img](./Markdown/Images/2024-04-29/img_25_2.png)


Attack performances when the attacker knows different portion of the victim model's training data.


EVALUATION


RQ3. What are the features of the training examples whose memberships are more likely to be correctly inferred?

MIA classifiers tend to perform better on examples that have lower perplexity scores. 
However, input length, output length, edit distance, and the number of variables show negligible effect sizes.


27


![img](./Markdown/Images/2024-04-29/img_26_1.png)


![img](./Markdown/Images/2024-04-29/img_27_1.png)




















Ë∞¢Ë∞¢Â§ßÂÆ∂ÁöÑÊâπËØÑÊåáÊ≠£ÔºÅ


Thank  You


![img](./Markdown/Images/2024-04-29/img_27_2.png)


28

