Improvement and optimization methods in image generation



Wuxia Bai
2024.03.25



Chen M, Radford A, Child R, et al. Generative pretraining from pixels[C]//International conference on machine learning. PMLR, 2020: 1691-1703.


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_1_1.png" width="1244" height="287"/>


3


Methodology


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_2_1.png" width="1176" height="384"/>


Motivation:
Supervised pre-training: curating large labeled image datasets is both expensive and time consuming. 
Unsupervised pre-training:  learn general purpose representations from the much larger set of available unlabeled images and fine-tune them for classification


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_2_2.png" width="280" height="94"/>


Architecture:


Context Reduction:
IRs: 32 × 32× 3, 48 ×48 × 3, or 64 ×64 × 3
motivated by early color display palettes, create 9-bit color palette by clustering (R, G, B) pixel values using k-means with k = 512.


4



Chen M, Radford A, Child R, et al. Generative pretraining from pixels[C]//International conference on machine learning. PMLR, 2020: 1691-1703.


Experiments and Results


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_3_1.png" width="595" height="517"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_3_2.png" width="589" height="611"/>


5



Chen M, Radford A, Child R, et al. Generative pretraining from pixels[C]//International conference on machine learning. PMLR, 2020: 1691-1703.


Experiments and Results


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_4_1.png" width="612" height="496"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_4_2.png" width="612" height="556"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_4_3.png" width="510" height="63"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_4_4.png" width="414" height="60"/>


6


Experiments and Results



Chen M, Radford A, Child R, et al. Generative pretraining from pixels[C]//International conference on machine learning. PMLR, 2020: 1691-1703.


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_5_1.png" width="568" height="481"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_5_2.png" width="530" height="514"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_5_3.png" width="308" height="67"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_5_4.png" width="176" height="69"/>


7


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_6_1.png" width="1201" height="305"/>



Meng C, He Y, Song Y, et al. Sdedit: Guided image synthesis and editing with stochastic differential equations[J]. arXiv preprint arXiv:2108.01073, 2021.


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_7_1.png" width="995" height="538"/>


8



Meng C, He Y, Song Y, et al. Sdedit: Guided image synthesis and editing with stochastic differential equations[J]. arXiv preprint arXiv:2108.01073, 2021.


Methodology


Motivation:
GAN-based methods: additional training data or loss functions for individual applications.
SDEdit:  does not require task-specific training or inversions


9


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_8_1.png" width="1069" height="524"/>


Methodology


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_8_2.png" width="915" height="88"/>



Meng C, He Y, Song Y, et al. Sdedit: Guided image synthesis and editing with stochastic differential equations[J]. arXiv preprint arXiv:2108.01073, 2021.


10


Methodology


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_9_1.png" width="1198" height="515"/>



Meng C, He Y, Song Y, et al. Sdedit: Guided image synthesis and editing with stochastic differential equations[J]. arXiv preprint arXiv:2108.01073, 2021.


11


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_10_1.png" width="895" height="346"/>



Meng C, He Y, Song Y, et al. Sdedit: Guided image synthesis and editing with stochastic differential equations[J]. arXiv preprint arXiv:2108.01073, 2021.


Methodology


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_10_2.png" width="760" height="35"/>


12



Meng C, He Y, Song Y, et al. Sdedit: Guided image synthesis and editing with stochastic differential equations[J]. arXiv preprint arXiv:2108.01073, 2021.


Experiments and Results


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_11_1.png" width="924" height="603"/>


13



Meng C, He Y, Song Y, et al. Sdedit: Guided image synthesis and editing with stochastic differential equations[J]. arXiv preprint arXiv:2108.01073, 2021.


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_12_1.png" width="1224" height="535"/>


Experiments and Results


14



Meng C, He Y, Song Y, et al. Sdedit: Guided image synthesis and editing with stochastic differential equations[J]. arXiv preprint arXiv:2108.01073, 2021.


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_13_1.png" width="972" height="603"/>


Experiments and Results


15



Meng C, He Y, Song Y, et al. Sdedit: Guided image synthesis and editing with stochastic differential equations[J]. arXiv preprint arXiv:2108.01073, 2021.


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_14_1.png" width="1220" height="479"/>


Experiments and Results


16



Meng C, He Y, Song Y, et al. Sdedit: Guided image synthesis and editing with stochastic differential equations[J]. arXiv preprint arXiv:2108.01073, 2021.


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_15_1.png" width="809" height="220"/>


Experiments and Results


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_15_2.png" width="822" height="318"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_15_3.png" width="456" height="364"/>


17


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_16_1.png" width="1076" height="353"/>


18


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_17_1.png" width="844" height="623"/>


Methodology


Karras T, Aittala M, Aila T, et al. Elucidating the design space of diffusion-based generative models[J]. Advances in Neural Information Processing Systems, 2022, 35: 26565-26577.


19


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_18_1.png" width="1165" height="355"/>


Experiments and Results


20


Advantages and disadvantages


1. Generative pretraining from pixels
Advantages: The first paper uses GPT pre-training method in image field
Disadvantages: The input is directly downsampled, and reshape is a bit simple and rough, losing a lot of information and the two-dimensional characteristics of the image

2. Sdedit: Guided image synthesis and editing with stochastic differential equations
Advantages: Without retraining, a single unconditional model can be used to solve problems such as conditional image generation, stroke based image composition and editing and other problems.
Disadvantages: It takes more time to generate a new image, as this iterative process takes much more time than a single pass through a more traditional GAN-based generation model.



