张智博


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_0_1.png" width="568" height="720"/>





2024.4.1


Enhancing Visual Media Generation:


Advanced Diffusion and Vision Transformer


Scalable Diffusion Models with Transformer


William Peebles*


Saining Xie


UC Berkeley


New York University


arXiv:2212.09748v2 [cs.CV] 2 Mar 2023


Scalable Diffusion Models with Transformer


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_2_1.png" width="517" height="327"/>


Motivation


使用 Transformer 替换扩散模型中的 U-net：DiTs
评估不同规模的 Transformer 扩散模型的性能


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_2_2.png" width="594" height="327"/>


Scalable Diffusion Models with Transformer


Method


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_3_1.png" width="780" height="471"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_3_2.png" width="454" height="362"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_3_3.png" width="393" height="378"/>


Scalable Diffusion Models with Transformer


Method


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_4_1.png" width="780" height="471"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_4_2.png" width="450" height="204"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_4_3.png" width="444" height="222"/>





Scalable Diffusion Models with Transformer


Method


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_5_1.png" width="780" height="471"/>





<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_5_2.png" width="444" height="295"/>


Scalable Diffusion Models with Transformer


Method


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_6_1.png" width="780" height="471"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_6_2.png" width="443" height="320"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_6_3.png" width="443" height="109"/>





Scalable Diffusion Models with Transformer


Experimental Setup


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_7_1.png" width="464" height="153"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_7_2.png" width="456" height="372"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_7_3.png" width="501" height="305"/>


Scalable Diffusion Models with Transformer


Result


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_8_1.png" width="542" height="436"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_8_2.png" width="566" height="249"/>


Scalable Diffusion Models with Transformer


Result


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_9_1.png" width="1054" height="481"/>


Scalable Diffusion Models with Transformer


Result


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_10_1.png" width="1037" height="455"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_11_1.png" width="773" height="429"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_11_2.png" width="506" height="549"/>


Scalable Diffusion Models with Transformer


Result


Photorealistic Video Generation with Diffusion Models


Agrim Gupta*


Lijun Yu


Stanford University


Google Research


arXiv:2312.06662v1 [cs.CV] 11 Dec 2023


Li Feifei


Motivation


Photorealistic Video Generation with Diffusion Models


Window Attention Latent Transformer (W.A.L.T): 
a transformer-based method for latent video diffusion models (LVDMs)


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_13_1.png" width="508" height="306"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_13_2.png" width="480" height="356"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_14_1.png" width="1154" height="449"/>


Method


Photorealistic Video Generation with Diffusion Models





<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_14_2.png" width="503" height="220"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_15_1.png" width="1154" height="449"/>


Method


Photorealistic Video Generation with Diffusion Models





<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_15_2.png" width="555" height="363"/>


* Agrim Gupta, Stephen Tian, Yunzhi Zhang, Jiajun Wu, and Li Fei-Fei. MaskViT: Masked visual pre-training for video prediction


Method


Photorealistic Video Generation with Diffusion Models


Self-Conditioning


* Ting Chen, Ruixiang Zhang, and Geoffrey Hinton. Analog
bits: Generating discrete data using diffusion models with
self-conditioning.


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_16_1.png" width="748" height="210"/>


Method


Photorealistic Video Generation with Diffusion Models


AdaLN-Lora


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_17_1.png" width="418" height="530"/>


Method


Photorealistic Video Generation with Diffusion Models


Video Super Resolution


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_18_1.png" width="420" height="472"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_18_2.png" width="406" height="186"/>


Result


Photorealistic Video Generation with Diffusion Models


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_19_1.png" width="1038" height="327"/>








Result


Photorealistic Video Generation with Diffusion Models


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_20_1.png" width="1185" height="500"/>


Result


Photorealistic Video Generation with Diffusion Models


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_21_1.png" width="678" height="468"/>


Patch n’ Pack: NaViT, a Vision Transformer
for any Aspect Ratio and Resolution


Mostafa Dehghani∗


Basil Mustafa∗


Google DeepMind


arXiv:2307.06304v1 [cs.CV] 12 Jul 2023


Patch n’ Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution


Motivation


ViT输入图像的大小需要调整为固定的纵横比。
NaViT在训练期间将来自不同图像的多个补丁打包在一个序列中，称为 Patch n’Pack，能够适应任意纵横比和分辨率的输入。
主流图片数据集中的绝大部分图片都不是正方形的。


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_23_1.png" width="1118" height="211"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_24_1.png" width="832" height="462"/>


Method


Patch n’ Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution





Continuous Token Dropping


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_24_2.png" width="608" height="74"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_25_1.png" width="832" height="462"/>


Method


Patch n’ Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution


Resolution sampling


原始分辨率训练：NaViT支持使用每个图像的原始分辨率进行训练，这样可以保留图像的所有细节。
保留纵横比的重新采样：作为一种替代方法，NaViT允许在保持图像原始纵横比的前提下，对图像的像素总数进行重新采样。这种方法提供了调整图像分辨率的灵活性，以适应不同的训练需求。





<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_26_1.png" width="832" height="462"/>


Method


Patch n’ Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution





Packing & 
Sequence-level padding 


将多个样本图片中的patch放在一个序列中进行训练，这种方法被称为Patch n’Pack
处理包含多个样本的序列时，最终序列的长度必须是固定的，以适应模型训练中的批处理操作。
使用Padding Token来填补空白。



<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_27_1.png" width="832" height="462"/>


Method


Patch n’ Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution





Factorized & Fractional positional embeddings


绝对位置嵌入(absolute embeddings)：


分数位置嵌入(fractional embeddings)：


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_28_1.png" width="832" height="462"/>


Method


Patch n’ Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution





Masked self attention & Masked pooling


进行了Token Packing操作后，需要使用额外的mask以防同一pack下的不同样本相互干扰。
在Self-Attention层引入了额外的Masked self attention。
在编码器的顶层引入Masked pooling，将每个样本中的Token汇总成一个单独的向量。


Result


Patch n’ Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_29_1.png" width="1116" height="496"/>


Result


Patch n’ Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_30_1.png" width="491" height="397"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_30_2.png" width="588" height="372"/>


Result


Patch n’ Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_31_1.png" width="835" height="234"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_31_2.png" width="779" height="210"/>


Result


Patch n’ Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_32_1.png" width="963" height="455"/>


Result


Patch n’ Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_33_1.png" width="1187" height="461"/>


感谢观看

