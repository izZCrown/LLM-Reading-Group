Exploring Advances in Image Generation and Recognition: Improved Denoising Diffusion Probabilistic Models and Vision Transformer


Improved Denoising Diffusion Probabilistic Models



Alex Nichol, Prafulla Dhariwal


https://arxiv.org/abs/2102.09672


Review——DDPM


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_2_1.png" width="674" height="118"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_2_2.png" width="490" height="123"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_2_3.png" width="118" height="27"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_2_4.png" width="134" height="27"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_2_5.png" width="446" height="160"/>


Bayes’s Theorem


Substitute (9) into (11)


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_2_6.png" width="421" height="52"/>


with loss


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_2_7.png" width="409" height="38"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_2_8.png" width="459" height="38"/>


Inproving log-likelihood (1)


Another kind of loss function for DDPM:



DDPMs can generate high-fidelity samples according to FID and Inception Score, they were unable to achieve competitive log-likelihoods with these models.


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_3_1.png" width="626" height="52"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_3_2.png" width="528" height="154"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_3_3.png" width="474" height="293"/>


Inproving log-likelihood (1)


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_4_1.png" width="567" height="68"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_4_2.png" width="596" height="43"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_4_3.png" width="626" height="52"/>


Inproving log-likelihood (2)


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_5_1.png" width="1161" height="112"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_5_2.png" width="611" height="85"/>


Inproving log-likelihood (2)


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_6_1.png" width="434" height="302"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_6_2.png" width="676" height="444"/>


Result of Imroving log-likelihood





<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_7_1.png" width="426" height="253"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_7_2.png" width="418" height="222"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_7_3.png" width="475" height="308"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_7_4.png" width="437" height="103"/>


Comparison to GANs





<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_8_1.png" width="602" height="310"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_8_2.png" width="511" height="578"/>


An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale


Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby


https://arxiv.org/abs/2010.11929


Vision Transformer——ViT





<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_10_1.png" width="694" height="422"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_10_2.png" width="720" height="121"/>


Evaluation Settings


Models: 



Datasets: ImageNet (with 1k classes and 1.3M images), ImageNet-21k(with 21k classes and 14M images), JFT (with 18k classes and 303M high-resolution images).

Benchmarks: ReaL labels, CIFAR-10/100, Oxford-IIIT Pets, Oxford Flowers-102, 19-task VTAB classification suite.


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_11_1.png" width="548" height="154"/>


Evaluation——Compared to SOTA





<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_12_1.png" width="750" height="233"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_12_2.png" width="1005" height="299"/>


Evaluation——Pre-trained Datasets





<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_13_1.png" width="1021" height="528"/>


Evaluation——Scaling Study





<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_14_1.png" width="1076" height="530"/>


ViViT: A Video Vision Transformer



Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lucic, Cordelia Schmid, Google Research



https://arxiv.org/abs/2103.15691


Architecture


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_16_1.png" width="585" height="243"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_16_2.png" width="539" height="331"/>


Uniform frame sampling


Tubelet embedding 


Transformer





<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_17_1.png" width="1156" height="463"/>


Factorised Encoder





<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_18_1.png" width="634" height="560"/>


Factorised Self-Attention


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_19_1.png" width="645" height="365"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_19_2.png" width="481" height="124"/>


Factorised Dot-Product Attention





<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_20_1.png" width="657" height="336"/>


Evaluation Settings


Models: Apply the same naming scheme to our models.



Datasets: 
Kinetics consists of 10-second videos sampled at 25fps from YouTube. We evaluate on both Kinetics 400 and 600, containing 400 and 600 classes respectively
Epic Kitchens-100 consists of egocentric videos capturing daily kitchen activities spanning 100 hours and 90 000 clips.
Moments in Time consists of 800 000, 3-second YouTube clips that capture the gist of a dynamic scene involving animals, objects, people, or natural phenomena.
Something-Something v2 (SSv2) contains 220 000 videos, with durations ranging from 2 to 6 seconds.



<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_21_1.png" width="478" height="134"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_21_2.png" width="478" height="134"/>


Vi


Vi


Vi


Ablation Study on Models





<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_22_1.png" width="554" height="257"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_22_2.png" width="531" height="266"/>


<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_22_3.png" width="674" height="226"/>


Comparison to SOTA





<img src="https://github.com/izZCrown/LLM-Reading-Group/tree/main/Markdown/Images/base_name/img_23_1.png" width="1280" height="551"/>


Conclusion


DDPM still needs to be improved. In order to improve the generation efficiency of the diffusion model, LDM proposes to transfer the diffusion space from image space to latent space.


ViT and ViViT are the first Transformer-based models proposed in the image and video processing respectively. ViT and ViViT show that Transformer can achieve competitive results in the image and video field.



Thanks!





Presented by lyx
2024.3.11

