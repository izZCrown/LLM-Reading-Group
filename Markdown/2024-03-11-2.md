Exploring Advances in Image Generation and Recognition: Improved Denoising Diffusion Probabilistic Models and Vision Transformer


Improved Denoising Diffusion Probabilistic Models



Alex Nichol, Prafulla Dhariwal


https://arxiv.org/abs/2102.09672


Review——DDPM


![img](./Markdown/Images/2024-03-11-2/img_2_1.png)


![img](./Markdown/Images/2024-03-11-2/img_2_2.png)


![img](./Markdown/Images/2024-03-11-2/img_2_3.png)


![img](./Markdown/Images/2024-03-11-2/img_2_4.png)


![img](./Markdown/Images/2024-03-11-2/img_2_5.png)


Bayes’s Theorem


Substitute (9) into (11)


![img](./Markdown/Images/2024-03-11-2/img_2_6.png)


with loss


![img](./Markdown/Images/2024-03-11-2/img_2_7.png)


![img](./Markdown/Images/2024-03-11-2/img_2_8.png)


Inproving log-likelihood (1)


Another kind of loss function for DDPM:



DDPMs can generate high-fidelity samples according to FID and Inception Score, they were unable to achieve competitive log-likelihoods with these models.


![img](./Markdown/Images/2024-03-11-2/img_3_1.png)


![img](./Markdown/Images/2024-03-11-2/img_3_2.png)


![img](./Markdown/Images/2024-03-11-2/img_3_3.png)


Inproving log-likelihood (1)


![img](./Markdown/Images/2024-03-11-2/img_4_1.png)


![img](./Markdown/Images/2024-03-11-2/img_4_2.png)


![img](./Markdown/Images/2024-03-11-2/img_4_3.png)


Inproving log-likelihood (2)


![img](./Markdown/Images/2024-03-11-2/img_5_1.png)


![img](./Markdown/Images/2024-03-11-2/img_5_2.png)


Inproving log-likelihood (2)


![img](./Markdown/Images/2024-03-11-2/img_6_1.png)


![img](./Markdown/Images/2024-03-11-2/img_6_2.png)


Result of Imroving log-likelihood





![img](./Markdown/Images/2024-03-11-2/img_7_1.png)


![img](./Markdown/Images/2024-03-11-2/img_7_2.png)


![img](./Markdown/Images/2024-03-11-2/img_7_3.png)


![img](./Markdown/Images/2024-03-11-2/img_7_4.png)


Comparison to GANs





![img](./Markdown/Images/2024-03-11-2/img_8_1.png)


![img](./Markdown/Images/2024-03-11-2/img_8_2.png)


An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale


Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby


https://arxiv.org/abs/2010.11929


Vision Transformer——ViT





![img](./Markdown/Images/2024-03-11-2/img_10_1.png)


![img](./Markdown/Images/2024-03-11-2/img_10_2.png)


Evaluation Settings


Models: 



Datasets: ImageNet (with 1k classes and 1.3M images), ImageNet-21k(with 21k classes and 14M images), JFT (with 18k classes and 303M high-resolution images).

Benchmarks: ReaL labels, CIFAR-10/100, Oxford-IIIT Pets, Oxford Flowers-102, 19-task VTAB classification suite.


![img](./Markdown/Images/2024-03-11-2/img_11_1.png)


Evaluation——Compared to SOTA





![img](./Markdown/Images/2024-03-11-2/img_12_1.png)


![img](./Markdown/Images/2024-03-11-2/img_12_2.png)


Evaluation——Pre-trained Datasets





![img](./Markdown/Images/2024-03-11-2/img_13_1.png)


Evaluation——Scaling Study





![img](./Markdown/Images/2024-03-11-2/img_14_1.png)


ViViT: A Video Vision Transformer



Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lucic, Cordelia Schmid, Google Research



https://arxiv.org/abs/2103.15691


Architecture


![img](./Markdown/Images/2024-03-11-2/img_16_1.png)


![img](./Markdown/Images/2024-03-11-2/img_16_2.png)


Uniform frame sampling


Tubelet embedding 


Transformer





![img](./Markdown/Images/2024-03-11-2/img_17_1.png)


Factorised Encoder





![img](./Markdown/Images/2024-03-11-2/img_18_1.png)


Factorised Self-Attention


![img](./Markdown/Images/2024-03-11-2/img_19_1.png)


![img](./Markdown/Images/2024-03-11-2/img_19_2.png)


Factorised Dot-Product Attention





![img](./Markdown/Images/2024-03-11-2/img_20_1.png)


Evaluation Settings


Models: Apply the same naming scheme to our models.



Datasets: 
Kinetics consists of 10-second videos sampled at 25fps from YouTube. We evaluate on both Kinetics 400 and 600, containing 400 and 600 classes respectively
Epic Kitchens-100 consists of egocentric videos capturing daily kitchen activities spanning 100 hours and 90 000 clips.
Moments in Time consists of 800 000, 3-second YouTube clips that capture the gist of a dynamic scene involving animals, objects, people, or natural phenomena.
Something-Something v2 (SSv2) contains 220 000 videos, with durations ranging from 2 to 6 seconds.



![img](./Markdown/Images/2024-03-11-2/img_21_1.png)


![img](./Markdown/Images/2024-03-11-2/img_21_2.png)


Vi


Vi


Vi


Ablation Study on Models





![img](./Markdown/Images/2024-03-11-2/img_22_1.png)


![img](./Markdown/Images/2024-03-11-2/img_22_2.png)


![img](./Markdown/Images/2024-03-11-2/img_22_3.png)


Comparison to SOTA





![img](./Markdown/Images/2024-03-11-2/img_23_1.png)


Conclusion


DDPM still needs to be improved. In order to improve the generation efficiency of the diffusion model, LDM proposes to transfer the diffusion space from image space to latent space.


ViT and ViViT are the first Transformer-based models proposed in the image and video processing respectively. ViT and ViViT show that Transformer can achieve competitive results in the image and video field.



Thanks!





Presented by lyx
2024.3.11

