2024-01-22


Research on Knowledge Editing for LLMs


![img](./Markdown/Images/2024-01-22/img_1_1.png)


Background


Model Editing
fix and update LLMs

Differences between editing and other techniques
Knowledge Editing: Precise control, difficult, may not effective;
Knowledge Augmented: Short-term change, poor scaling, retrieval noise;
Continual Learning: Easy to overfit, affect other knowledge, heavy overhead

Model Editing Evaluation
Efficacy
Generality
Locality



![img](./Markdown/Images/2024-01-22/img_1_2.png)


![img](./Markdown/Images/2024-01-22/img_1_3.png)


![img](./Markdown/Images/2024-01-22/img_1_4.png)


Model Editing


Preserve Models’ Parameters
Memory-based Model
Additional Parameters


Modify Models’ Parameters
Locate-Then-Edit
Meta-learning


![img](./Markdown/Images/2024-01-22/img_2_1.png)


![img](./Markdown/Images/2024-01-22/img_3_1.png)


![img](./Markdown/Images/2024-01-22/img_3_2.png)


Approach (1):  Preserve Models’ Parameters


Memory-based Model
store all edit examples explicitly in memory
use retrievers to extract the most relevant edit facts
SERAC, MemPrompt, IKE, MeLLo
SERAC
Counterfactual Model
Scope Classifier

Additional Parameters
use extra parameters trained on a modified knowledge dataset
original model parameters remain static
T-Patcher, CaliNET, GRACE


Approach (2): Modify Models’ Parameters


Meta-learning
use a hyper network to learn the necessary weights
Knowledge Edit, MEND




![img](./Markdown/Images/2024-01-22/img_4_1.png)


![img](./Markdown/Images/2024-01-22/img_4_2.png)


Approach (2): Modify Models’ Parameters


Locate-Then-Edit
identify specific parameters and modify them through direct updates
Knowledge Neuron (KN), ROME, MEMIT



![img](./Markdown/Images/2024-01-22/img_5_1.png)


![img](./Markdown/Images/2024-01-22/img_5_2.png)


![img](./Markdown/Images/2024-01-22/img_5_3.png)


Approach (2): Modify Models’ Parameters


Locate-Then-Edit
identify specific parameters and modify them through direct updates
Knowledge Neuron (KN), ROME, MEMIT



![img](./Markdown/Images/2024-01-22/img_6_1.png)


![img](./Markdown/Images/2024-01-22/img_6_2.png)


![img](./Markdown/Images/2024-01-22/img_6_3.png)


Editing Evaluation


Single-editing
make a single editing operation (one instance/batch)
Sequential-editing
conduct multiple operations successively
Instance-editing
using only one instance per editing operation
Batch-editing
update hundreds or thousands of facts in one batch


![img](./Markdown/Images/2024-01-22/img_7_1.png)


Experiments


Editing Methods
MEND, KN, ROME, MEMIT
Datasets
Zero-Shot Relation Extraction (ZSRE) 
Selected LLMs
GPT-2 XL (1.5B), LLaMA-1 (7B)
Tasks and Metrics
Reasoning on the GSM8K
Natural language inference (NLI) on the RTE
Open-domain QA on the Natural Question
Closed-domain QA on the BoolQ 
Dialogue on the MuTual
Summarization on the SAMSum
Named entity recognition (NER) on the CoNLL03
Sentiment analysis on the SST2


![img](./Markdown/Images/2024-01-22/img_8_1.png)


![img](./Markdown/Images/2024-01-22/img_9_1.png)


Evaluation


Impact of Instance- and Sequential-editing
editing 1 instance per operation
use KN/ROME
a downward trend as the number 
of edits increases
Llama-1 are not robust to weight updates. 



![img](./Markdown/Images/2024-01-22/img_10_1.png)


Evaluation


Impact of Batch Size on Editing
use MEND/MEMIT
In most cases, the larger the batch size, the worse the model performs on various tasks


![img](./Markdown/Images/2024-01-22/img_10_2.png)


Evaluation


Impact of Batch- and Sequential-editing


![img](./Markdown/Images/2024-01-22/img_11_1.png)


![img](./Markdown/Images/2024-01-22/img_11_2.png)


![img](./Markdown/Images/2024-01-22/img_11_3.png)


![img](./Markdown/Images/2024-01-22/img_11_4.png)


Gradually & Catastrophic Forgetting


Using ROME/MEMIT
Models under test: GPT2-XL
Gradually Forgetting
Catastrophic Forgetting




![img](./Markdown/Images/2024-01-22/img_12_1.png)


![img](./Markdown/Images/2024-01-22/img_12_2.png)


![img](./Markdown/Images/2024-01-22/img_12_3.png)


References


A Comprehensive Study of Knowledge Editing for Large Language Models (https://arxiv.org/abs/2401.01286)
Editing Large Language Models: Problems, Methods, and Opportunities (https://arxiv.org/abs/2305.13172)
Knowledge Neurons in Pretrained Transformers (https://arxiv.org/abs/2104.08696)
Model Editing Can Hurt General Abilities of Large Language Models (https://arxiv.org/abs/2401.04700)
Model Editing at Scale leads to Gradual and Catastrophic Forgetting (https://arxiv.org/abs/2401.07453)





Thanks for your listening!

