


    


Insights on Interpretability in Large Language Models


![img](./Markdown/Images/2024-04-08/img_0_1.png)


![img](./Markdown/Images/2024-04-08/img_0_2.png)


![img](./Markdown/Images/2024-04-08/img_0_3.png)


![img](./Markdown/Images/2024-04-08/img_0_4.png)


![img](./Markdown/Images/2024-04-08/img_0_5.png)





![img](./Markdown/Images/2024-04-08/img_1_1.png)


![img](./Markdown/Images/2024-04-08/img_1_2.png)





![img](./Markdown/Images/2024-04-08/img_1_3.png)





![img](./Markdown/Images/2024-04-08/img_2_1.png)


![img](./Markdown/Images/2024-04-08/img_2_2.png)





Background


![img](./Markdown/Images/2024-04-08/img_2_3.png)


Hallucination
The generated content from LLMs is nonsensical, unfaithful, or incorrect in response to a given query
Possible Reasons
Source and Target Discrepancies
Unintentional Source-Target Discrepancies
Intentional Source-Target Discrepancies
Repetitiveness in Training Data
Impact of Data Noise
Randomness in the Decoding Process
Model's Parameter Knowledge Bias
Decoding Differences between Training and Actual Application





![img](./Markdown/Images/2024-04-08/img_3_1.png)


![img](./Markdown/Images/2024-04-08/img_3_2.png)





Methodology



![img](./Markdown/Images/2024-04-08/img_3_3.png)


Providing two inputs to the LLM
Extracting threecritical hidden states to be analyzed
S1 represents the final hidden state of the question segment
S2 relates to the final hidden state of the entire hallucinated input
S3 is obtained for the correct input.





![img](./Markdown/Images/2024-04-08/img_4_1.png)


![img](./Markdown/Images/2024-04-08/img_4_2.png)





 Experiment Setup


Datasets
TruthfulQA
https://huggingface.co/datasets/truthful_qa
HaluEval
https://huggingface.co/datasets/pminervini/HaluEval
LLMs
LLaMA-2 7B
LLaMA-2-Chat-7B
LLaMA-2 13B






![img](./Markdown/Images/2024-04-08/img_5_1.png)


![img](./Markdown/Images/2024-04-08/img_5_2.png)





 Empirical Findings


![img](./Markdown/Images/2024-04-08/img_5_3.png)


![img](./Markdown/Images/2024-04-08/img_5_4.png)


![img](./Markdown/Images/2024-04-08/img_5_5.png)


![img](./Markdown/Images/2024-04-08/img_5_6.png)


awareness score = 


![img](./Markdown/Images/2024-04-08/img_5_7.png)





![img](./Markdown/Images/2024-04-08/img_6_1.png)


![img](./Markdown/Images/2024-04-08/img_6_2.png)





 Empirical Findings


![img](./Markdown/Images/2024-04-08/img_6_3.png)


![img](./Markdown/Images/2024-04-08/img_6_4.png)


Pro-prompting boosts the LLM's certainty in the accurate answer and introduces skepticism towards the hallucinated response
Anti-prompting induces doubt in the correct answer while fostering confidence in the hallucinated one.






![img](./Markdown/Images/2024-04-08/img_7_1.png)


![img](./Markdown/Images/2024-04-08/img_7_2.png)





 Empirical Findings


![img](./Markdown/Images/2024-04-08/img_7_3.png)


![img](./Markdown/Images/2024-04-08/img_7_4.png)


![img](./Markdown/Images/2024-04-08/img_7_5.png)





![img](./Markdown/Images/2024-04-08/img_8_1.png)


![img](./Markdown/Images/2024-04-08/img_8_2.png)





 Empirical Findings


![img](./Markdown/Images/2024-04-08/img_8_3.png)





![img](./Markdown/Images/2024-04-08/img_9_1.png)


![img](./Markdown/Images/2024-04-08/img_9_2.png)





 Empirical Findings


![img](./Markdown/Images/2024-04-08/img_9_3.png)


![img](./Markdown/Images/2024-04-08/img_9_4.png)





![img](./Markdown/Images/2024-04-08/img_10_1.png)


![img](./Markdown/Images/2024-04-08/img_10_2.png)





 Conclusions


The study reveals that LLMs' hidden states vary between accurate and hallucinated responses.
A framework was introduced to analyze LLMs' awareness of hallucination, focusing on the LLaMA model family.
Empirical evidence suggests the potential of using LLMs' hidden representations to reduce hallucinations.
Insights from this research could enhance the reliability of LLMs in important applications.





![img](./Markdown/Images/2024-04-08/img_11_1.png)


![img](./Markdown/Images/2024-04-08/img_11_2.png)





![img](./Markdown/Images/2024-04-08/img_11_3.png)





![img](./Markdown/Images/2024-04-08/img_12_1.png)


![img](./Markdown/Images/2024-04-08/img_12_2.png)





 Background


![img](./Markdown/Images/2024-04-08/img_12_3.png)


Jailbreak
Large Language Models (LLMs) are vulnerable to 'Jailbreaking' prompts, a type of attack that can coax these models into generating harmful and illegal content. 
This study investigates the effects of pruning on the safety alignment of large language models (LLMs).





![img](./Markdown/Images/2024-04-08/img_13_1.png)


![img](./Markdown/Images/2024-04-08/img_13_2.png)





Methodology


![img](./Markdown/Images/2024-04-08/img_13_3.png)


Datasets
225 hypothetical malicious tasks
Five categories
Each category has 45 tasks divided into low, medium, and high severity






![img](./Markdown/Images/2024-04-08/img_14_1.png)


![img](./Markdown/Images/2024-04-08/img_14_2.png)





Methodology


![img](./Markdown/Images/2024-04-08/img_14_3.png)


Models
LLaMA-2-Chat
Vicuna 1.3
Mistral Instruct v0.2
Response Evaluation
Refused
Incomplete
Correct







![img](./Markdown/Images/2024-04-08/img_15_1.png)


![img](./Markdown/Images/2024-04-08/img_15_2.png)





Methodology
Wanda Pruning


![img](./Markdown/Images/2024-04-08/img_15_3.png)


[1] Sun M, Liu Z, Bair A, et al. A simple and effective pruning approach for large language models[J]. arXiv preprint arXiv:2306.11695, 2023.





![img](./Markdown/Images/2024-04-08/img_16_1.png)


![img](./Markdown/Images/2024-04-08/img_16_2.png)


Results


![img](./Markdown/Images/2024-04-08/img_16_3.png)


![img](./Markdown/Images/2024-04-08/img_16_4.png)





![img](./Markdown/Images/2024-04-08/img_17_1.png)


![img](./Markdown/Images/2024-04-08/img_17_2.png)





Results


![img](./Markdown/Images/2024-04-08/img_17_3.png)





![img](./Markdown/Images/2024-04-08/img_18_1.png)


![img](./Markdown/Images/2024-04-08/img_18_2.png)





Results


![img](./Markdown/Images/2024-04-08/img_18_3.png)





![img](./Markdown/Images/2024-04-08/img_19_1.png)


![img](./Markdown/Images/2024-04-08/img_19_2.png)





 Conclusions



Pruning up to 20% of LLM parameters increases resistance to jailbreaking prompts without hindering performance.
Enhanced safety post-pruning correlates with the model's initial safety training, suggesting a more generalizable impact of pruning.
A new dataset of 225 harmful tasks across five categories was used to demonstrate that pruning helps LLMs focus on relevant information in jailbreaking scenarios.
Popular models like LLaMA-2 Chat, Vicuna, and Mistral Instruct show high susceptibility to jailbreaking, with success rates of 70-100% in some cases.
Pruning offers a promising method to improve LLM safety and reliability, with potential applications in other aspects of LLM behavior.

